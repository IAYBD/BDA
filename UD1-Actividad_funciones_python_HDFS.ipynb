{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56709555-db98-460e-88b7-41e7febaa4d4",
   "metadata": {},
   "source": [
    "## Nombre alumno: Rafael Navarro Gómez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b153b3",
   "metadata": {},
   "source": [
    "# Práctica HDFS: Interacción con HDFS desde Python\n",
    "\n",
    "En esta práctica se utilizará la clase `InsecureClient` del paquete `hdfs` para realizar operaciones básicas en HDFS: crear directorios, subir y descargar archivos, listar contenido y obtener información de archivos.\n",
    "\n",
    "**Nota:** Los directorios iniciales están vacíos, por lo que todas las operaciones de listado o lectura comienzan con la creación de directorios y archivos necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff61e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la librería\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "\n",
    "# Conexión al HDFS\n",
    "client = InsecureClient('http://localhost:9870', user='root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded48cb",
   "metadata": {},
   "source": [
    "## Crear directorios en HDFS\n",
    "\n",
    "Primero creamos un directorio de usuario y un subdirectorio donde trabajaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127cdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.makedirs('/user/hadoop_user')\n",
    "client.makedirs('/user/hadoop_user/datos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c645292",
   "metadata": {},
   "source": [
    "## Crear archivos en HDFS\n",
    "\n",
    "Vamos a crear algunos archivos de ejemplo directamente desde Python para poder trabajar con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96704cbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "/user/hadoop_user/datos/archivo1.txt for client 127.0.0.1 already exists\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:389)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2732)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:807)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/user/hadoop_user/datos/archivo1.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContenido del archivo 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m client\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop_user/datos/archivo2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContenido del archivo 2\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:520\u001b[0m, in \u001b[0;36mClient.write\u001b[0;34m(self, hdfs_path, data, overwrite, permission, blocksize, replication, buffersize, append, encoding)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m AsyncWriter(consumer)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m   \u001b[43mconsumer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:515\u001b[0m, in \u001b[0;36mClient.write.<locals>.consumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    509\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    510\u001b[0m   method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m append \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    511\u001b[0m   url\u001b[38;5;241m=\u001b[39mloc,\n\u001b[1;32m    512\u001b[0m   data\u001b[38;5;241m=\u001b[39m(c\u001b[38;5;241m.\u001b[39mencode(encoding) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m _data) \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;28;01melse\u001b[39;00m _data,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[0;32m--> 515\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _to_error(res)\n",
      "\u001b[0;31mHdfsError\u001b[0m: /user/hadoop_user/datos/archivo1.txt for client 127.0.0.1 already exists\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:389)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2732)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:807)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n"
     ]
    }
   ],
   "source": [
    "client.write('/user/hadoop_user/datos/archivo1.txt', data='Contenido del archivo 1', encoding='utf-8')\n",
    "client.write('/user/hadoop_user/datos/archivo2.txt', data='Contenido del archivo 2', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba16d60",
   "metadata": {},
   "source": [
    "## Listar contenido de directorios\n",
    "\n",
    "Primero listamos los archivos sin información adicional, luego con información de estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45ee3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archivo1.txt', 'archivo2.txt']\n",
      "\n",
      "[('archivo1.txt', {'accessTime': 1760634564904, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16400, 'group': 'supergroup', 'length': 23, 'modificationTime': 1760634565463, 'owner': 'root', 'pathSuffix': 'archivo1.txt', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}), ('archivo2.txt', {'accessTime': 1760634565482, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16401, 'group': 'supergroup', 'length': 23, 'modificationTime': 1760634565514, 'owner': 'root', 'pathSuffix': 'archivo2.txt', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'})]\n"
     ]
    }
   ],
   "source": [
    "# Listado simple\n",
    "print(client.list('/user/hadoop_user/datos'))\n",
    "!echo \"\"\n",
    "# Listado con información de estado\n",
    "print(client.list('/user/hadoop_user/datos', status=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311356f",
   "metadata": {},
   "source": [
    "## Leer un archivo desde HDFS\n",
    "\n",
    "Usamos `pandas` para leer el contenido de un archivo si fuera un CSV. Aquí usamos texto simple como ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47d16220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del archivo 1\n"
     ]
    }
   ],
   "source": [
    "with client.read('/user/hadoop_user/datos/archivo1.txt') as reader:\n",
    "    contenido = reader.read().decode('utf-8')\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca73ee",
   "metadata": {},
   "source": [
    "## Descargar archivos desde HDFS\n",
    "\n",
    "Descargamos un archivo a la máquina local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da3f58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/archivo1_descargado.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.download('/user/hadoop_user/datos/archivo1.txt', './archivo1_descargado.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60f02a",
   "metadata": {},
   "source": [
    "## Subir archivos desde local a HDFS\n",
    "\n",
    "Creamos un archivo local y lo subimos a HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff095b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/hadoop_user/datos/archivo_local_subido.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear archivo local\n",
    "with open('archivo_local.txt', 'w') as f:\n",
    "    f.write('Contenido del archivo local')\n",
    "\n",
    "# Subirlo a HDFS\n",
    "client.upload('/user/hadoop_user/datos/archivo_local_subido.txt', 'archivo_local.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a729dbe",
   "metadata": {},
   "source": [
    "## Obtener información de archivos\n",
    "\n",
    "Ver información de permisos, tamaño y propietario de un archivo en HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c5ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accessTime': 1760634564904, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16400, 'group': 'supergroup', 'length': 23, 'modificationTime': 1760634565463, 'owner': 'root', 'pathSuffix': '', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}\n"
     ]
    }
   ],
   "source": [
    "info = client.status('/user/hadoop_user/datos/archivo1.txt')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd4dcf",
   "metadata": {},
   "source": [
    "## Eliminar archivos y directorios\n",
    "\n",
    "Primero eliminamos archivos individuales, luego eliminamos un directorio completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16ce483e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete('/user/hadoop_user/datos/archivo2.txt')\n",
    "client.delete('/user/hadoop_user/datos', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3cdc4",
   "metadata": {},
   "source": [
    "# Ejercicios de práctica para desarrollar\n",
    "\n",
    "A continuación se presentan 10 ejercicios que el alumno debe desarrollar completamente, usando las funciones vistas en esta práctica. Cada ejercicio requiere crear los directorios y archivos necesarios antes de operar sobre ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a57f56",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Crear un directorio `/user/hadoop_user/proyectos` en HDFS y dentro de él, un subdirectorio llamado `2025`. Luego, crear un archivo de texto `informe.txt` dentro de `/user/hadoop_user/proyectos/2025` con contenido de tu elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b80a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución del Ejercicio 1\n",
    "client.delete('/user/hadoop_user/proyectos', recursive=True)\n",
    "client.makedirs(\"/user/hadoop_user/proyectos\")\n",
    "client.makedirs(\"/user/hadoop_user/proyectos/2025\")\n",
    "client.write('/user/hadoop_user/proyectos/2025/informe.txt', data='Buenas tardes desde Hadoop', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2e177",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Lista los archivos y subdirectorios de `/user/hadoop_user/proyectos` antes y después de crear un archivo llamado `resumen.txt` en ese mismo directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "078fd365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de resumen.txt\n",
      "['2025']\n",
      "['informe.txt']\n",
      "\n",
      "\n",
      "Después de resumen.txt\n",
      "['2025', 'resumen.txt']\n"
     ]
    }
   ],
   "source": [
    "# Solución del Ejercicio 2\n",
    "print(\"Antes de resumen.txt\")\n",
    "print(client.list('/user/hadoop_user/proyectos'))\n",
    "print(client.list('/user/hadoop_user/proyectos/2025'))\n",
    "!echo \"\"\n",
    "client.write('/user/hadoop_user/proyectos/resumen.txt', data='Esto es un resumen', encoding='utf-8')\n",
    "!echo \"\"\n",
    "print(\"Después de resumen.txt\")\n",
    "print(client.list('/user/hadoop_user/proyectos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b35df0",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Crea un archivo local llamado `datos_cliente.csv` y súbelo a HDFS en `/user/hadoop_user/proyectos/2025`. Asegúrate de que el directorio destino existe antes de subir el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a631825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/hadoop_user/proyectos/2025/datos_cliente.csv'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solución del Ejercicio 3\n",
    "# Crear archivo local\n",
    "with open('datos_cliente.csv', 'w') as f:\n",
    "    f.write('Hola,Mundo,Desde,Local')\n",
    "\n",
    "# Subirlo a HDFS\n",
    "client.upload('/user/hadoop_user/proyectos/2025/datos_cliente.csv', 'datos_cliente.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1489bc",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "Descarga el archivo `datos_cliente.csv` que subiste desde HDFS al directorio local `./descargas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709d895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/datos_cliente_descargado.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solución del Ejercicio 4\n",
    "client.download('/user/hadoop_user/proyectos/2025/datos_cliente.csv', './datos_cliente_descargado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3caab",
   "metadata": {},
   "source": [
    "### Ejercicio 5\n",
    "Lee el contenido del archivo `/user/hadoop_user/proyectos/2025/informe.txt` directamente desde HDFS y muéstralo por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ae48f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buenas tardes desde Hadoop\n"
     ]
    }
   ],
   "source": [
    "# Solución del Ejercicio 5\n",
    "with client.read('/user/hadoop_user/proyectos/2025/informe.txt') as reader:\n",
    "    contenido = reader.read().decode('utf-8')\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e30fa2",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "Consulta los permisos, propietario y tamaño del archivo `/user/hadoop_user/proyectos/2025/informe.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d14b97ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accessTime': 1760635415100, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16408, 'group': 'supergroup', 'length': 26, 'modificationTime': 1760635415129, 'owner': 'root', 'pathSuffix': '', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}\n"
     ]
    }
   ],
   "source": [
    "# Solución del Ejercicio 6\n",
    "info = client.status('/user/hadoop_user/proyectos/2025/informe.txt')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452691d",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "Elimina el archivo `/user/hadoop_user/proyectos/2025/informe.txt` de HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2161052c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solución del Ejercicio 7\n",
    "client.delete('/user/hadoop_user/proyectos/2025/informe.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbf198",
   "metadata": {},
   "source": [
    "### Ejercicio 8\n",
    "Elimina todo el directorio `/user/hadoop_user/proyectos/2025` de manera recursiva, incluyendo todos los archivos que contenga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef86da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solución del Ejercicio 8\n",
    "client.delete('/user/hadoop_user/proyectos/2025', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ab6c7",
   "metadata": {},
   "source": [
    "### Ejercicio 9\n",
    "Crea un directorio `/user/hadoop_user/reportes` y dentro de él, tres archivos de texto (`r1.txt`, `r2.txt`, `r3.txt`). Luego lista el contenido con información de estado (`status=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbc0bf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r1.txt', {'accessTime': 1760637328061, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16412, 'group': 'supergroup', 'length': 24, 'modificationTime': 1760637328095, 'owner': 'root', 'pathSuffix': 'r1.txt', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}), ('r2.txt', {'accessTime': 1760637328106, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16413, 'group': 'supergroup', 'length': 24, 'modificationTime': 1760637328136, 'owner': 'root', 'pathSuffix': 'r2.txt', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}), ('r3.txt', {'accessTime': 1760637328146, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16414, 'group': 'supergroup', 'length': 24, 'modificationTime': 1760637328178, 'owner': 'root', 'pathSuffix': 'r3.txt', 'permission': '644', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'})]\n"
     ]
    }
   ],
   "source": [
    "# Solución del Ejercicio 9\n",
    "client.makedirs('/user/hadoop_user/reportes')\n",
    "client.write('/user/hadoop_user/reportes/r1.txt', data='Contenido del archivo r1', encoding='utf-8')\n",
    "client.write('/user/hadoop_user/reportes/r2.txt', data='Contenido del archivo r2', encoding='utf-8')\n",
    "client.write('/user/hadoop_user/reportes/r3.txt', data='Contenido del archivo r3', encoding='utf-8')\n",
    "print(client.list('/user/hadoop_user/reportes', status=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ccafc",
   "metadata": {},
   "source": [
    "### Ejercicio 10\n",
    "Crea un directorio `/user/hadoop_user/mis_pruebas`, dentro de él un subdirectorio `prueba_python`. Dentro de `prueba_python`: crear un archivo `ejemplo1.txt`, subir un archivo local `archivo_local.txt`, listar los archivos, leer `ejemplo1.txt`, descargar `archivo_local.txt` a `./descargas_pruebas` y eliminar finalmente `ejemplo1.txt` y todo el subdirectorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dae97e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Remote path '/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:580\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m   statuses \u001b[38;5;241m=\u001b[39m [status \u001b[38;5;28;01mfor\u001b[39;00m _, status \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HdfsError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:1124\u001b[0m, in \u001b[0;36mClient.list\u001b[0;34m(self, hdfs_path, status)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(statuses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;129;01mnot\u001b[39;00m statuses[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpathSuffix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus(hdfs_path)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1121\u001b[0m   \u001b[38;5;66;03m# HttpFS behaves incorrectly here, we sometimes need an extra call to\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;66;03m# make sure we always identify if we are dealing with a file.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m ):\n\u001b[0;32m-> 1124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m HdfsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m is not a directory.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status:\n",
      "\u001b[0;31mHdfsError\u001b[0m: '/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt' is not a directory.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchivo_local.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEste es mi archivo local\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marchivo_local.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(client\u001b[38;5;241m.\u001b[39mlist(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop_user/mis_pruebas/prueba_python\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop_user/mis_pruebas/prueba_python/ejemplo1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:585\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot a directory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m err\u001b[38;5;241m.\u001b[39mmessage:\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;66;03m# Remote path is a normal file.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HdfsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRemote path \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoes not exist\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m err\u001b[38;5;241m.\u001b[39mmessage:\n\u001b[1;32m    587\u001b[0m   \u001b[38;5;66;03m# Remote path doesn't exist.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m   temp_path \u001b[38;5;241m=\u001b[39m hdfs_path\n",
      "\u001b[0;31mHdfsError\u001b[0m: Remote path '/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt' already exists."
     ]
    }
   ],
   "source": [
    "# Solución del Ejercicio 10\n",
    "client.makedirs('/user/hadoop_user/mis_pruebas')\n",
    "client.makedirs('/user/hadoop_user/mis_pruebas/prueba_python')\n",
    "client.write('/user/hadoop_user/mis_pruebas/prueba_python/ejemplo1.txt', data='Esto es solo un ejemplo', encoding='utf-8')\n",
    "# Crear archivo local\n",
    "with open('archivo_local.txt', 'w') as f:\n",
    "    f.write('Este es mi archivo local')\n",
    "\n",
    "client.upload('/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt', 'archivo_local.txt')\n",
    "print(client.list('/user/hadoop_user/mis_pruebas/prueba_python'))\n",
    "with client.read('/user/hadoop_user/mis_pruebas/prueba_python/ejemplo1.txt') as reader:\n",
    "    contenido = reader.read().decode('utf-8')\n",
    "print(contenido)\n",
    "client.download('/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt', './archivo_local_descargado.txt')\n",
    "client.delete('/user/hadoop_user/mis_pruebas/prueba_python/archivo_local.txt')\n",
    "client.delete('/user/hadoop_user/mis_pruebas/prueba_python', recursive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
